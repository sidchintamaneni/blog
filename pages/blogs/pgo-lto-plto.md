# Exploring & Experimenting with PGO-LTO-PLTO

The idea of this blog is not to provide a lot of details of each of these
techniques but form an abstract mental model and experiment with them. If you
feel like I am skipping detailed explanantions that is on purpose please
checkout the references.

The details regarding the experimental setup are towards the end of the blog.

## How it all started

It all started when I am downloading CachyOS to run some sched_ext[1]
experiments. While the OS was downloading, I looked into one of their published
blogs - "CachyOS Recap 2026 and Merry Christmas"[2]. One thing that caught my
eye was the feature that they've recently introduced

```
Optimization: The default kernel (linux-cachyos) is now optimized using
Propeller in conjunction with AutoFDO. This combination results in approximately
a 10% throughput improvement and reduced latency, depending on the workload.
```

The performance improvements looks insane. So now the question becomes what are
AutoFDO, Propeller and when did linux kernel started to support it? Which
workloads did they optimize? To answer the second question, I got to know with
a quick search that it was added to linux kernel build system in 6.13[3] and
currently only supported by clang/llvm compiler[4].

Now coming back to our first question what are AutoFDO & Propeller, I found a
self-explanatory presentation about Optimizing the Linux Kernel using AutoFDO &
Propeller[5]. Now I got introduced new terms - FDO, iFDO, AutoFDO, BOLF &
Propeller. I was barely familiar with FDO/PGO (Feedback directed
optimization/ Profile Guided optimization). We will explore what each of these
terms means and experiment with them.

I am expecting this blog to be a two part series. In the first part we will run
and experiment with these optimizations with a simple binary, later we will
discuss these in the context of Linux kernel. I didn't forget about our third
question & probably try to answer them in third blog.

## Short Intro to Compiler, Linker & their Optimizations

For ppl, who are not familiar with how modern compilers like llvm work. This is
how I formed a mental model about them. There are three layers frontend,
intermediate representation and backend. Frontend parses the source code (things
like lexer, parser, AST generation, Semantic analysis etc comes in this stage)
and generates an intermediate represenation. IR stage is the place where all
the compiler optimizations are applied, when you supply flags -O2/ -O3. Backend
generates the object files based on architecture.

After compiler generates the object files, linker is responsible for generating
the binary. llvm linker (lld) offers LTO (link time optimization & Thin LTO)
[7].  LTO is not performed on the obj files generated by backend instead it get
all the IR files (bitcode files) and generates one monolithic file and then
perform the optimizations, later invokes backend and linker subsequently.  One
can imagine that this is a memory intensive process. Fun fact I've done this in
the past to generate a callgraph for subset of kernel functions[8] and never
saw our lab servers using that much memory(It exhausted 256 GB and little bit
of swap that we had on those servers). To avoid this memory intensive ppl
have come up with ThinLTO. 

TODO: Write about ThinLTO

<also a segway to PGO>

## Profile Guided Optimization/ Feedback Directed Optimization (PGO/ FDO)

The idea of PGO is to collect profiling data from a program during runtime and
feed it the compiler to optimize the code more (inlining, code layout
optimizatioms, etc).

There are currently two ways todo FDO one is iFDO and other is AutoFDO[9].
Both of them differ by how they profile/ sample the data. iFDO instruments the
code to collect the data which is the reason why it is not used in production
and the data is collected over the simulated test workload. While AutoFDO
utilizes intel LBR (last branch records) which has less overhead because cpus
log data directly to model-specific registers (MSR) (only have read overhead,
checkout the reference[6] for more details) and used in production.

Let's talk about PLTO, BOLT and Propeller a little later in the blog. For now
let's experiment with LTO and PGO.

# Experimenting with LTO & PGO

For this blog, I am chosing the exisiting benchmarks from llvm-test-suite[10],
especially the tests under SingleSource/ & MultiSource/.

First I am running tests under SingleSource/ with three levels of compiler
optimizations and LTO.

Reference command to run llvm-test-suite to run with different compiler flags
```
cmake -G Ninja -DCMAKE_C_COMPILER=clang -DCMAKE_CXX_COMPILER=clang++
-DCMAKE_C_FLAGS="-O0" -DCMAKE_CXX_FLAGS="-O0" -DCMAKE_BUILD_TYPE=Release
-DTEST_SUITE_SUBDIRS="SingleSource" -DTEST_SUITE_RUN_BENCHMARKS=ON

ninja -j `nproc`

lit -v -j1 -o {file_name} SingleSource/Benchmarks/Shootout
```
I am trying to understand the results from SingleSource/Benchmarks/Shootout
folder since they are a bit easier to comprehend.

link to raw data: <hyperlink to experiment 1 rawdata table>




# References
[1] https://github.com/sched-ext/scx/tree/main
[2] https://cachyos.org/blog/2025-christmas-new-year/
[3] https://lore.kernel.org/all/20241102175115.1769468-1-xur@google.com/
[4] https://discourse.llvm.org/t/optimizing-the-linux-kernel-with-autofdo-including-thinlto-and-propeller/79108
[5] https://lpc.events/event/18/contributions/1922/attachments/1450/3084/AutoFDO%20&%20Propeller%20in%20LPC%202024.pdf
<!--Reference to LBR (last branch records) -->
[6] https://lwn.net/Articles/680985/
[7] https://blog.llvm.org/2016/06/thinlto-scalable-and-incremental-lto.html
[8] https://github.com/rosalab/callgraph_generatorV2
[9] https://github.com/google/autofdo
[10] https://github.com/llvm/llvm-test-suite/tree/main

## Experimental Setup
```
> lscpu
Vendor ID:                   GenuineIntel
  Model name:                Intel(R) Core(TM) i7-8700T CPU @ 2.40GHz
    CPU family:              6
    Model:                   158
    Thread(s) per core:      1
    Core(s) per socket:      6
    Socket(s):               1
    Stepping:                10
    CPU(s) scaling MHz:      33%
    CPU max MHz:             2400.0000
    CPU min MHz:             800.0000
Caches (sum of all):
  L1d:                       192 KiB (6 instances)
  L1i:                       192 KiB (6 instances)
  L2:                        1.5 MiB (6 instances)
  L3:                        12 MiB (1 instance)

> cat /etc/os-release
NAME="CachyOS Linux"
PRETTY_NAME="CachyOS"
ID=cachyos
BUILD_ID=rolling // live life dangerously

> uname -r
6.18.6-2-cachyos

❯ clang --version
clang version 21.1.6
Target: x86_64-pc-linux-gnu
Thread model: posix
InstalledDir: /usr/bin

❯ llvm-config --version
21.1.6
```
